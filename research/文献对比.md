#网页信息提取 文献总结-差异和对比

## 基于DOM树进行正文识别
1. 叶子节点融合的相关算法   
    `《Web Content Information Extraction Based on DOM Tree and Statistical Information》`
    - GetRealNode() 先获取子节点再进行融合， 直至公共的祖先节点`CommonAncestor`   
    - 结合节点中的超链接文本的个数、长度以及层级，为每一个节点Node Block进行打分（判断是否是重要节点）
    - 设置阈值区分内容块和噪声块
    
2. 基于DOM树结构相似性的正文节点提取   
    `《Web Content Extraction Using Clustering with Web Structure》`
    - 对于相似的DOM结构，静态内容常为一些噪声块，动态内容为主要内容。
    - 通过聚类算法把有监督的训练变成无监督的训练
    - 缺点在于有些噪声块也可以为动态内容

3. 基于节点相似性进行聚类    
    `《Web Content Extraction Through Machine Learning》`   
    - 由于不同网页之间设计和布局存在较大差异，选择了DBSCAN作为聚类算法来解决簇数目未知/簇形状未知/噪声等问题（未给出距离函数）。
    - 通过与标题简介等meta信息对比文本间的差异（最长公共子序列LCS算法），来评估每一个集群与描述之间的相似性。
    - 比较适合新闻类这样的标题涵盖了主要信息的网站
    
## 基于模板进行正文识别   
主要的步骤是
- 形成一个模板库
- 对比网页的dom结构和模板，计算相似度
- 选择模板进行信息提取

## 基于机器学习进行模式识别
- 非常依赖特定的模式组合   
- 输入一个需要被识别的模式样本库，比如商品模块，依靠训练让机器记住这种模式
- 样本库包含两个部分
    - 视觉信息（网页截图）
    - 文本信息（文本编码）
- 从类似的网页中提取对应模式的数据（从候选框中选取样本，将其坐标投影到最终的特征向量，然后再用softmax将其分类）


## 基于机器学习进行区域定位
`《Deep web data extraction based on visual information processing》`
- 使用CNN对网页的截图进行卷积
- 使用类VIPS算法进行视觉块生成
- 结合两者进行区域的识别

## 基于语义标签进行正文提取
- 并非所有的网页都是具有<content>标签的H5页面
- 大部分的网页其内容还是会被包裹在DIV这样的标签中。

## 基于词库进行信息提取
- 大致过程就是通过词向量归纳相类似的词，比如电影领域的词库，然后对DOM节点进行配对，猜测是否为目标节点。
- 结合RNN构建一个信息抽取的模型，对节点进行标记
- 首先需要获取一定数量的主题型页面（比如电影页面），并对用户指定的关键目标信息进行标记
- 然后使用的标记过的样本页面进行训练，使系统获得识别目标信息的能力
- 网页内容预处理（手动预处理，筛节点，然后添加一些关键词标记）

- 缺点是需要构建词库并进行手动标记